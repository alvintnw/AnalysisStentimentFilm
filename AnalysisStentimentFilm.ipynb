{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQ5X4zu8flTKd3n2gceZH+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvintnw/AnalysisStentimentFilm/blob/main/AnalysisStentimentFilm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0Fgcm_ZhQnO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re # Untuk regular expression (pembersihan teks)\n",
        "import nltk # Natural Language Toolkit\n",
        "from nltk.corpus import stopwords # Untuk menghapus kata-kata umum\n",
        "from nltk.stem import PorterStemmer # Untuk stemming (mengurangi kata ke akar)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Untuk mengubah teks menjadi angka\n",
        "from sklearn.linear_model import LogisticRegression # Model klasifikasi\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Unduh resource NLTK yang dibutuhkan (hanya perlu sekali)\n",
        "# Jika ada error, coba jalankan ini di sel terpisah:\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt') # Untuk tokenisasi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# URL dataset (raw CSV dari GitHub)\n",
        "# Dataset ini berisi ulasan film dan label sentimennya (positif/negatif)\n",
        "url = \"https://raw.githubusercontent.com/Ankit152/IMDB-Sentiment-Analysis/master/IMDB-Dataset.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Menampilkan 5 baris pertama dari DataFrame\n",
        "print(\"5 baris pertama dari dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Menampilkan informasi dasar tentang dataset\n",
        "print(\"\\nInformasi dataset:\")\n",
        "df.info()\n",
        "\n",
        "# Menampilkan jumlah ulasan positif dan negatif\n",
        "print(\"\\nDistribusi Sentimen:\")\n",
        "print(df['sentiment'].value_counts())"
      ],
      "metadata": {
        "id": "efcdXm3Uhc9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fb063ea"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi Porter Stemmer dan stop words\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english')) # Dataset ini dalam bahasa Inggris\n",
        "\n",
        "def clean_text(text):\n",
        "    # 1. Hapus tag HTML\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # 2. Hapus karakter non-alfabet dan ubah ke huruf kecil\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
        "    # 3. Tokenisasi (pisahkan teks menjadi kata-kata)\n",
        "    words = text.split()\n",
        "    # 4. Hapus stop words dan lakukan stemming\n",
        "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "    # 5. Gabungkan kembali kata-kata menjadi string\n",
        "    text = ' '.join(words)\n",
        "    return text\n",
        "\n",
        "# Terapkan fungsi pembersihan ke kolom 'review'\n",
        "df['cleaned_review'] = df['review'].apply(clean_text)\n",
        "\n",
        "# Menampilkan beberapa ulasan asli dan yang sudah dibersihkan\n",
        "print(\"Contoh Ulasan Asli vs Dibersihkan:\")\n",
        "for i in range(5):\n",
        "    print(f\"Asli: {df['review'][i][:100]}...\") # Ambil 100 karakter pertama\n",
        "    print(f\"Dibersihkan: {df['cleaned_review'][i][:100]}...\\n\")"
      ],
      "metadata": {
        "id": "8l9BHLCmh5qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengubah 'positive' menjadi 1 dan 'negative' menjadi 0\n",
        "df['sentiment_numeric'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
        "\n",
        "# Menampilkan 5 baris pertama dengan kolom sentimen numerik baru\n",
        "print(df.head())\n",
        "print(\"\\nDistribusi Sentimen Numerik:\")\n",
        "print(df['sentiment_numeric'].value_counts())"
      ],
      "metadata": {
        "id": "YZBcdJTEiZ1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['cleaned_review']\n",
        "y = df['sentiment_numeric']\n",
        "\n",
        "print(\"Bentuk X (Fitur):\", X.shape)\n",
        "print(\"Bentuk y (Target):\", y.shape)"
      ],
      "metadata": {
        "id": "C7T8hoauieHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Jumlah data pelatihan: {len(X_train)} ({len(X_train)/len(df)*100:.2f}%)\")\n",
        "print(f\"Jumlah data pengujian: {len(X_test)} ({len(X_test)/len(df)*100:.2f}%)\")\n",
        "print(\"\\nDistribusi sentimen dalam data pelatihan:\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "print(\"\\nDistribusi sentimen dalam data pengujian:\")\n",
        "print(y_test.value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "0fWzOBtHiieu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000) # Batasi jumlah fitur (kata)\n",
        "\n",
        "# Pelajari kosakata dari data pelatihan dan ubah teks menjadi vektor TF-IDF\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Ubah data pengujian menggunakan kosakata yang sama\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(\"Bentuk X_train_tfidf:\", X_train_tfidf.shape)\n",
        "print(\"Bentuk X_test_tfidf:\", X_test_tfidf.shape)"
      ],
      "metadata": {
        "id": "LpNNCWGUimlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat instance model Regresi Logistik\n",
        "model = LogisticRegression(max_iter=1000) # Tingkatkan max_iter jika konvergensi tidak tercapai\n",
        "\n",
        "print(\"Model Regresi Logistik berhasil diinisialisasi!\")"
      ],
      "metadata": {
        "id": "IYpBCBjjisdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melatih model menggunakan data pelatihan yang sudah di-TF-IDF\n",
        "print(\"Melatih model...\")\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "print(\"Model berhasil dilatih!\")"
      ],
      "metadata": {
        "id": "rC_KxVcuivLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat prediksi pada data pengujian\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Prediksi pada data pengujian berhasil dibuat!\")"
      ],
      "metadata": {
        "id": "pO2MXkUfix6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung Akurasi\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Akurasi Model: {accuracy:.4f}\\n\")\n",
        "\n",
        "# Menampilkan Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
        "\n",
        "# Menampilkan Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel('Prediksi')\n",
        "plt.ylabel('Aktual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dkOtyqZgi2kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text):\n",
        "    # 1. Bersihkan teks input\n",
        "    cleaned_text = clean_text(text)\n",
        "    # 2. Ubah teks menjadi vektor TF-IDF menggunakan vectorizer yang sudah dilatih\n",
        "    text_tfidf = tfidf_vectorizer.transform([cleaned_text])\n",
        "    # 3. Buat prediksi\n",
        "    prediction = model.predict(text_tfidf)\n",
        "    # 4. Kembalikan label sentimen\n",
        "    if prediction[0] == 1:\n",
        "        return \"Positif\"\n",
        "    else:\n",
        "        return \"Negatif\"\n",
        "\n",
        "# Contoh ulasan baru\n",
        "ulasan1 = \"This movie was absolutely fantastic! I loved every minute of it.\"\n",
        "ulasan2 = \"What a terrible film. I wasted my money and time.\"\n",
        "ulasan3 = \"The acting was okay, but the plot was a bit confusing.\"\n",
        "ulasan4 = \"A masterpiece of cinema, truly breathtaking.\"\n",
        "ulasan5 = \"I wouldn't recommend this to anyone. Very disappointing.\"\n",
        "\n",
        "print(f\"Ulasan 1: '{ulasan1}' -> Sentimen: {predict_sentiment(ulasan1)}\")\n",
        "print(f\"Ulasan 2: '{ulasan2}' -> Sentimen: {predict_sentiment(ulasan2)}\")\n",
        "print(f\"Ulasan 3: '{ulasan3}' -> Sentimen: {predict_sentiment(ulasan3)}\")\n",
        "print(f\"Ulasan 4: '{ulasan4}' -> Sentimen: {predict_sentiment(ulasan4)}\")\n",
        "print(f\"Ulasan 5: '{ulasan5}' -> Sentimen: {predict_sentiment(ulasan5)}\")"
      ],
      "metadata": {
        "id": "7Abe27hWi7v4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}